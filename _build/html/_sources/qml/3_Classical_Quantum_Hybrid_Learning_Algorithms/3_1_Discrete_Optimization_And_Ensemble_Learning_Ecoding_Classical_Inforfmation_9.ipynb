{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "55fhGyOMr9Ga"
   },
   "source": [
    "# Encoding Classical Information\n",
    "\n",
    "Irrespective of the choice of a quantum computing paradigm, you will always face the problem-- how do I encode the classical information I have in the quantum system to perform some quantum protocol? So there are a couple of ways of doing it, and here's an overview of all of them. The easiest thing to do is to do something called basis encoding. \n",
    "\n",
    "In this case, you don't do anything different from what you do on a digital computer. So say you have the number three that you would like to encode. Then you would write it as binary. So this would be the representation of digital computing where this is bit zero and this is bit one. And then you just take this, and you write it as a quantum state. \n",
    "\n",
    "$$x=3\\Rightarrow11\\Rightarrow\\mid11\\rangle$$\n",
    "\n",
    "You will use two qubits, and you would flip both of them to one. And it would describe the number three. And if you have a vector, say, of these two elements, you can create a binary vector of that. And then you can concatenate the two strings into a single ket, into a four qubit state in this case. \n",
    "\n",
    "$$x=\\left[\\begin{array}{c}\n",
    "3\\\\\n",
    "2\n",
    "\\end{array}\\right]\\Rightarrow\\mid1110\\rangle$$\n",
    "\n",
    "The greatest advantage of this encoding is that it's very easy to prepare because you only have to flip certain qubits. Most random computers start from being initialized in the zero state. And then you just have to make these not operation, the x operations, to get to the state that you want to express. But the great disadvantage is that it's very wasteful of your qubits. You need lots and lots of qubits to describe, say, floating point numbers. Another way of doing it is amplitude encoding. \n",
    "\n",
    "$$x=\\left[\\begin{array}{c}\n",
    "x_{0}\\\\\n",
    "x_{1}\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "$$\\parallel x\\parallel_{2}=1$$\n",
    "\n",
    "Imagine that you have a vector. And let's have this assumption that your vector is already normalized. Its length is one. Then what you can do is just write down the same thing with the original coefficients, indexing the individual coefficients of probability amplitudes with the basis vector. \n",
    "\n",
    "$$\\mid x\\rangle=x_{0}\\mid0\\rangle+x_{1}\\mid1\\rangle$$\n",
    "\n",
    "The advantage of this is that it requires a lot fewer qubits. And in principle, at least in theory, this is of infinite precision. So if you have a real number here, you would have a real number here. But the disadvantage is that it's unclear exactly how you would prepare this state and how you would read out the actual values here. So you would have to dream up some state preparation protocol, and then you would have to perform tomography to understand what these probability aptitudes are at the end of your calculation. Then they can also encode the problem in a Hamiltonian. So when you think about it, there are actually two ways of doing this. One is that we have seen over and over again this is the ising way. \n",
    "\n",
    "$$H=-\\sum_{<i,j>}J_{ij}\\sigma_{i}\\sigma_{j}+\\sum_{i}h_{i}\\sigma_{i}$$\n",
    "\n",
    "You have a problem. You method to the ising model, or our quadratic unconstrained binary optimization problem is given in this form. And that's what you saw, for instance, by quantum annealing. So the encoding comes in in these couplings, in these biased terms J_{ij}, h_{i}. These are the ones expressing your problem. Whereas here \\mid1110\\rangle, it was the state. And here $x_{0}$, $x_{1}$, it will have all the probability amplitudes. So the advantage of this is that it's fairly easy to implant. We know we are up to thousands of qubits in a physical system implementing this model. But the disadvantage is that it has a very limited scope in what you can do. You can solve either optimization of sampling problems with this paradigm. Then the second way of doing Hamiltonian encoding is by doing Hamiltonian simulation. \n",
    "\n",
    "$$U=e^{-iHt}$$\n",
    "\n",
    "Now this simulation is a bit misleading because this is not a simulation on the classical digital computer. This is a quantum computer simulating a quantum system. So what you are doing is actually you're trying to implement this unitary on a quantum computer. And an example of this is exactly what the QAOA optimization algorithm does when it approximates the adiabatic pathway. And a very, very important subroutine in many idealized quantum machine learning algorithms is quantum matrix inversion. And it does the exact same thing. It encodes the matrix to be encoded here in the Hamiltonian. So the advantage of this is exactly this. It's very natural to encode the matrix in this formalism. The disadvantage is that there are countless terms and conditions that apply. So we only see quantum matrix inversion in the part that we talk about coherent quantum protocols and large scale quantum computers. And its very, very limited what we can do on actual quantum computers today when we want to use this representation. \n",
    "\n",
    "Check\n",
    "\n",
    "• Why can't we encode everything in bit strings as in digital computing?\n",
    "\n",
    "– This would be inefficient.\n",
    "\n",
    "– Some optimization problems naturally map to the Ising model.\n",
    "\n",
    "– The probability amplitudes naturally encode real and complex numbers.\n",
    "\n",
    "• Mapping to the Ising model is both Hamiltonian and basis encoding, since the spin variables have to express a part of your problem.\n",
    "\n",
    "– True\n",
    "\n",
    "• Hamiltonian simulation is efficient for any Hamiltonian. \n",
    "\n",
    "– False\n",
    "\n",
    "#F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2HFoar1TUoK"
   },
   "source": [
    "Any learning algorithm will always have strengths and weaknesses: a single model is unlikely to fit every possible scenario. Ensembles combine multiple models to achieve higher generalization performance than any of the constituent models is capable of. How do we assemble the weak learners? We can use some sequential heuristics. For instance, given the current collection of models, we can add one more based on where that particular model performs well. Alternatively, we can look at all the correlations of the predictions between all models, and optimize for the most uncorrelated predictors. Since this latter is a global approach, it naturally maps to a quantum computer. But first, let's take a look a closer look at loss functions and regularization, two key concepts in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTDfRr1ETUoa"
   },
   "source": [
    "# Loss Functions and Regularization\n",
    "\n",
    "If you can solve a problem by a classical computer -- let that be a laptop or a massive GPU cluster -- there is little value in solving it by a quantum computer that costs ten million dollars. The interesting question in quantum machine learning is whether there are problems in machine learning and AI that fit quantum computers naturally, but are challenging on classical hardware. This, however, requires a good understanding of both machine learning and contemporary quantum computers.\n",
    "\n",
    "In this course, we primarily focus on the second aspect, since there is no shortage of educational material on classical machine learning. However, it is worth spending a few minutes on going through some basics.\n",
    "\n",
    "Let us take a look at the easiest possible problem: the data points split into two, easily distinguishable sets. We randomly generate this data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "whqbMFpsTUoc",
    "outputId": "a8dd118e-171e-4eb9-f8ae-0e189432de05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x106503070>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "c1 = np.random.rand(50, 2)/5\n",
    "c2 = (-0.6, 0.5) + np.random.rand(50, 2)/5\n",
    "data = np.concatenate((c1, c2))\n",
    "labels = np.array([0] * 50 + [1] *50)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.subplot(111, xticks=[], yticks=[])\n",
    "plt.scatter(data[:50, 0], data[:50, 1], color='navy')\n",
    "plt.scatter(data[50:, 0], data[50:, 1], color='c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1YzECo9TUof"
   },
   "source": [
    "Let's shuffle the data set into a training set that we are going to optimize over (2/3 of the data), and a test set where we estimate our generalization performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nYVy-j0JTUog",
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = np.arange(len(labels))\n",
    "np.random.shuffle(idx)\n",
    "# train on a random 2/3 and test on the remaining 1/3\n",
    "idx_train = idx[:2*len(idx)//3]\n",
    "idx_test = idx[2*len(idx)//3:]\n",
    "X_train = data[idx_train]\n",
    "X_test = data[idx_test]\n",
    "y_train = labels[idx_train]\n",
    "y_test = labels[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gv87SdKgTUog"
   },
   "source": [
    "We will use the package `scikit-learn` to train various machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "E75-Agy8TUoh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.metrics\n",
    "metric = sklearn.metrics.accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JzRG3RKTUoi"
   },
   "source": [
    "Let's train a perceptron, which has a linear loss function $\\frac{1}{N}\\sum_{i=1}^N |h(x_i)-y_i)|$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8GDjOmcrTUoi",
    "outputId": "b7754be4-0b7d-488f-98f3-95cdf492b6c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  1.00\n",
      "accuracy (test):  1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "model_1 = Perceptron(max_iter=1000, tol=1e-3)\n",
    "model_1.fit(X_train, y_train)\n",
    "print('accuracy (train): %5.2f'%(metric(y_train, model_1.predict(X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, model_1.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-Js4gjaTUoj"
   },
   "source": [
    "It does a great job. It is a linear model, meaning its decision surface is a plane. Our dataset is separable by a plane, so let's try another linear model, but this time a support vector machine. If you eyeball our dataset, you will see that to define the separation between the two classes, actually only a few points close to the margin are relevant. These are called support vectors and support vector machines aim to find them. Its objective function measures the loss and it has a regularization term with a weight $C$. The $C$ hyperparameter controls a regularization term that penalizes the objective for the number of support vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2i5fsPKbTUok",
    "outputId": "d3dc4f11-3334-4cda-b27d-5094be645764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  1.00\n",
      "accuracy (test):  1.00\n",
      "Number of support vectors: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model_2 = SVC(kernel='linear', C=10)\n",
    "model_2.fit(X_train, y_train)\n",
    "print('accuracy (train): %5.2f'%(metric(y_train, model_2.predict(X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, model_2.predict(X_test))))\n",
    "print('Number of support vectors:', sum(model_2.n_support_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlolB0l2TUol"
   },
   "source": [
    "It picks only two datapoints out of the hundred. Let's change the hyperparameter to reduce the penalty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wWx_ygFFTUol",
    "outputId": "87d958c5-9510-4002-dc3b-37b911aee1ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  0.52\n",
      "accuracy (test):  0.47\n",
      "Number of support vectors: 64\n"
     ]
    }
   ],
   "source": [
    "model_2 = SVC(kernel='linear', C=0.01)\n",
    "model_2.fit(X_train, y_train)\n",
    "print('accuracy (train): %5.2f'%(metric(y_train, model_2.predict(X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, model_2.predict(X_test))))\n",
    "print('Number of support vectors:', sum(model_2.n_support_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt72ZQhiTUom"
   },
   "source": [
    "You can see that the model gets confused by using too many datapoints in the final classifier. This is one example where regularization helps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6LFnJImqtNeP"
   },
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "So far we have gone through how to encode classical information in quantum computers. And we are getting ready to start talking about how you do learning on the encoded information in a quantum computer. But first, let's talk about a couple of things in machine learning. So when we talk about optimization in machine learning, we typically don't talk about the kind of optimization that you would perform on a quantum computer. So in a typical scenario, you're given a sample. \n",
    "\n",
    "$$S=\\{(x_{i},y_{i})\\}_{i=1}^{N}\\qquad x\\in\\mathbb{R}^{d}$$\n",
    "\n",
    "You're given a couple of data points in some high dimensional space each coming with some label. Labels can be 0 or 1 or belonging to some finite many classes. So what you're actually learning in the machine learning model that you're trying to fit to the data is conditional probability distribution. \n",
    "\n",
    "$$\\underbrace{h(\\theta,x)}:\\mathbb{R}^{d}\\rightarrow\\{0,1\\}$$\n",
    "\n",
    "$$P(y\\mid x):\\mathbb{R}^{d}\\rightarrow\\{0,c-1\\}$$\n",
    "\n",
    "So you're trying to approximate that, you know, once you see a data instance of a certain form, what is the label that you should predict? So this type of learning is discriminative and supervised, and this is exactly where methods like deep learning excel. \n",
    "\n",
    "$$\\underset{\\theta}{min}L(g,S)$$\n",
    "\n",
    "And what you do is you define a loss function in the parameters that you're trying to optimize for the particular machine learning model. And it's also a function of the actual sample that you are given. And then you look at these parameters, these theta parameters. They're sitting in some high dimensional space themselves.\n",
    "\n",
    "$$\\theta\\in\\mathbb{R}^{d}$$\n",
    "\n",
    "And each one of the entries of these theta vector is actually a 32 or a 64-bit precision floating point number. That's how it is represented on a computer, a digital computer. And when we talk about things like deep learning, then this perimeter space is extremely high dimensional. We are talking about millions of weights. So you use at least 32 bits per rate, and you have millions of them, whereas the largest quantum computer we have has 2,000 qubits. So there seems to be a misfit between this continuous type of optimization and what we can do on a quantum computer. So we have to think a little bit differently. There is hope you can use quantum computers for optimization but not for these kind of problems. So let's take a look at ensembles. \n",
    "\n",
    "$$h_{1}(\\theta_{1};x),\\ldots,h_{k}(\\theta_{k};x)$$\n",
    "\n",
    "$$F_{k}(w,x)=\\sum_{k=1}^{K}w_{k}h_{k}(\\theta_{k},x)$$\n",
    "\n",
    "So in ensembles, for instance, you can take a couple of large neural networks $h_{1}(\\theta_{1};x),\\ldots,h_{k}(\\theta_{k};x)$. And what you want to do is you want to combine them into a single strong predictor $\\sum_{k=1}^{K}w_{k}h_{k}(g_{k},x)$. So for instance, each of these neural networks gets a couple of instances wrong, and there will be other neural networks who would compensate for that mistake. And how you combine them is a big open question. So you want to weight each and every one of the neural networks so their overall prediction is actually stronger. And when you look at this, this starts to get a discrete flavor because now you have discrete many neural networks. And while this combination of weights is real valued or continuous valued, what actually matters is the relative importance of each of these neural networks so there is a way to discretize it better. So this idea of ensembles have been around for at least 20 years, slightly more, and they are still very important. Like, many of the Kaggle competitions are won by ensembles of neural networks. And one of the first algorithms that made this idea extremely useful was AdaBoost. So what it does, it keeps expanding this ensemble sequentially. So it adds a new learning model to the ensemble one by one.\n",
    "\n",
    "$$F_{m}(w,x)=F_{m-1}(w,x)+w_{m}h_{m}(g_{m},x)$$\n",
    "\n",
    "So if you have less than the available models, then what you do is you take the previous ensemble $F_{m-1}(w,x)$ that you ensembled so far. And you add the new model $w_{m}h_{m}(g_{m},x)$ with some corresponding weight. And the way you calculate the weight of this new model is by looking at the exponential loss, you factorize this exponential loss into two parts-- the loss coming from the previous ensemble and the loss coming from the new addition.\n",
    "\n",
    "$$\\sum_{i=i}^{N}e^{-y:F_{m}(x_{i})}$$\n",
    "\n",
    "And it's a good model then if you have a higher weight. And if it's not so good model if you receive a lower rate. \n",
    "\n",
    "Regularization is absent, which means that if you use up all of your K available models. So even, if something is very bad, it will still be in the ensemble. And if its predictor is strongly correlated with one of the other predictors, it's still going to be there even though it doesn't really contribute anything new. Nevertheless, there's all sorts of modern variants which address, for instance, this problem. One is called xg boost, and the other is called gradient boosted trees. These are very important algorithms. Even in today's machine learning, these are actively used. And it's worth looking at boosting methods and ensemble learning and see how quantum computers can help. \n",
    "\n",
    "Check\n",
    "\n",
    "• A supervised discriminative learning algorithm...\n",
    "\n",
    "– Trains on a training set with labels: $\\{(x_{i},y_{i})\\}_{i=1}^{N}$\n",
    "\n",
    "– Approximates a conditional probability distribution $P(y\\mid x)$.\n",
    "\n",
    "• You are training a linear classifier $$h(\\theta,x)=\\theta_{0}+\\theta_{1}x_{1}+\\ldots\\theta_{d}x_{d}$$ on a sample $$S=\\{(x_{i},y_{i})\\}_{i=1}^{N}$$ where $\\theta\\in\\mathbb{R}^{d+1}$ and . You threshold the model y to get a class for some input x . What's the most straightforward way to train this model?\n",
    "\n",
    "– Define a differentiable loss function and do a gradient descent.\n",
    "\n",
    "• Why do we expect an ensemble to work better than a single predictor?\n",
    "\n",
    "– The ensemble is easier to train.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHc0B7sgTUon"
   },
   "source": [
    "# Ensemble methods\n",
    "\n",
    "Ensembles yield better results when there is considerable diversity among the base classifiers. If diversity is sufficient, base classifiers make different errors, and a strategic combination may reduce the total error, ideally improving generalization performance. A constituent model in an ensemble is also called a base classifier or weak learner, and the composite model a strong learner.\n",
    "\n",
    "The generic procedure of ensemble methods has two steps. First, develop a set of base classifiers from the training data. Second, combine them to form the ensemble. In the simplest combination, the base learners vote, and the label prediction is based on majority. More involved methods weigh the votes of the base learners. \n",
    "\n",
    "Let us import some packages and define our figure of merit as accuracy in a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.000793Z",
     "start_time": "2018-11-19T20:10:17.128450Z"
    },
    "id": "CXyXGHX2TUoo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "%matplotlib inline\n",
    "\n",
    "metric = sklearn.metrics.accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UH6tG1NTUop"
   },
   "source": [
    "We generate a random dataset of two classes that form concentric circles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.174692Z",
     "start_time": "2018-11-19T20:10:18.003641Z"
    },
    "id": "oaPDkQz1TUop",
    "outputId": "f0452bd5-4425-4797-decd-b0992e9ef376"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2881c3dc0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "data, labels = sklearn.datasets.make_circles()\n",
    "idx = np.arange(len(labels))\n",
    "np.random.shuffle(idx)\n",
    "# train on a random 2/3 and test on the remaining 1/3\n",
    "idx_train = idx[:2*len(idx)//3]\n",
    "idx_test = idx[2*len(idx)//3:]\n",
    "X_train = data[idx_train]\n",
    "X_test = data[idx_test]\n",
    "\n",
    "y_train = 2 * labels[idx_train] - 1  # binary -> spin\n",
    "y_test = 2 * labels[idx_test] - 1\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "normalizer = sklearn.preprocessing.Normalizer()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "X_test = normalizer.fit_transform(X_test)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.subplot(111, xticks=[], yticks=[])\n",
    "plt.scatter(data[labels == 0, 0], data[labels == 0, 1], color='navy')\n",
    "plt.scatter(data[labels == 1, 0], data[labels == 1, 1], color='c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYrIAFunTUoq"
   },
   "source": [
    "Let's train a perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.226327Z",
     "start_time": "2018-11-19T20:10:18.177561Z"
    },
    "id": "hRYiAtR7TUor",
    "outputId": "1c969d6c-6f7b-4fe3-d9fc-0ca0aeffeddd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  0.44\n",
      "accuracy (test):  0.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "model_1 = Perceptron(max_iter=1000, tol=1e-3)\n",
    "model_1.fit(X_train, y_train)\n",
    "print('accuracy (train): %5.2f'%(metric(y_train, model_1.predict(X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, model_1.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsD9oHX1TUor"
   },
   "source": [
    "Since its decision surface is linear, we get a poor accuracy. Would a support vector machine with a nonlinear kernel fare better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.244639Z",
     "start_time": "2018-11-19T20:10:18.230025Z"
    },
    "id": "aSCLdCb-TUos",
    "outputId": "d20300a0-bc25-4e2d-b631-2f20bcd69ef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  0.64\n",
      "accuracy (test):  0.24\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model_2 = SVC(kernel='rbf', gamma='auto')\n",
    "model_2.fit(X_train, y_train)\n",
    "print('accuracy (train): %5.2f'%(metric(y_train, model_2.predict(X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, model_2.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgemS6dhTUos"
   },
   "source": [
    "It performs better on the training set, but at the cost of extremely poor generalization. \n",
    "\n",
    "Boosting is an ensemble method that explicitly seeks models that complement one another. The variation between boosting algorithms is how they combine weak learners. Adaptive boosting (AdaBoost) is a popular method that combines the weak learners in a sequential manner based on their individual accuracies. It has a convex objective function that does not penalize for complexity: it is likely to include all available weak learners in the final ensemble. Let's train AdaBoost with a few weak learners:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.314089Z",
     "start_time": "2018-11-19T20:10:18.248869Z"
    },
    "id": "sqNd7LyETUot",
    "outputId": "cd046c69-c89b-49b0-a061-a9c0aa9f63d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  0.65\n",
      "accuracy (test):  0.29\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model_3 = AdaBoostClassifier(n_estimators=3)\n",
    "model_3.fit(X_train, y_train)\n",
    "print('accuracy (train): %5.2f'%(metric(y_train, model_3.predict(X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, model_3.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0dCrg77TUot"
   },
   "source": [
    "Its performance is marginally better than that of the SVM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vi8X4Rwb1FXh"
   },
   "source": [
    "# Qboost\n",
    "Let's continue our exploration of ensemble methods. But this time around, let's take a look at what we can do with a quantum computer. So when we talked about AdaBoost, we mentioned that we used an exponential loss, and there was no regularization term. So let's take a look at how we can come up with a different objective function that would map better to a quantum computer and that would give some kind of an advantage over the classical method. So we are given a sample of data points. \n",
    "\n",
    "$$S=\\{(x_{i},y_{i})\\}_{i=1}^{N}$$\n",
    "\n",
    "And data points, the points themselves lie in some high dimensional space, and they come with binary labels in this case. And then we are also given a couple of models $\\{h_{k}(x)\\}_{k=1}^{K}$ which were already learned. So you can think about it, for instance, as large neural networks. And we have capital $K$ of them. \n",
    "\n",
    "$$\\underset{w}{min}\\left[\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\sum_{k=1}^{K}\\boldsymbol{w}_{z}h_{z}(x_{i})-y_{i}\\right)^{z}+\\lambda\\parallel W\\parallel_{0}\\right]$$\n",
    "\n",
    "$$\\parallel W\\parallel\\in\\mathbb{R}^{K}\\qquad\\text{not discrete}$$\n",
    "\n",
    "$$\\parallel W\\parallel_{0}=\\sqrt{\\sum W_{i}^{0}}$$\n",
    "\n",
    "So now what we do is we measure the square loss between the prediction of the ensemble, which combines the individual models, and the actual label yi. So this contrasts with the exponential loss that we talked about in AdaBoost. So that's the actual loss part. And we have a second part here which does a regularization $\\lambda\\parallel W\\parallel_{0}$. \n",
    "\n",
    "So we take the zero norm of the W weight vector. So the zero norm just measures how many elements are non-zero. So what it does is if we increase the value of this hyperparameter $\\lambda$, then it becomes extremely important. You had to have most of the W entries zero. On the other end, if you decrease lambda, then there's a lower penalty for including more and more elements in this ensemble. So zero means that the model is not included and the non-zero value means that the particular model will be part of the ensemble. So this way, we can find a trade-off between having a simple model and having some additions to the ensemble that may or may not improve our overall prediction. So in principle, these W weights are not discrete. So we still talk about real values. But what really matters is the relative importance and whether they are included. So we can reduce the bit width. So we can use just, say, three widths to represent a weight. And now we have a discrete problem. Now we have to transform it a little bit and use our Hamiltonian encoding, namely that-- and one of the two kinds of Hamiltonian encodings, the Ising encoding. \n",
    "\n",
    "$$\\underset{w}{min}\\left[\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\sum_{k=1}^{K}\\boldsymbol{w}_{z}h_{z}(x_{i})\\right)^{z}-2\\sum_{k=1}^{K}\\boldsymbol{w}_{z}h_{z}(x_{i})y_{i}+y_{i}^{2}\\boldsymbol{w}_{z}h_{z}(x_{i})+\\lambda\\parallel W\\parallel_{0}\\right]$$\n",
    "\n",
    "$$=\\underset{w}{min}\\left[\\frac{1}{N}\\sum_{i=1}^{N}\\boldsymbol{w}_{k}\\boldsymbol{w}_{l}\\left(\\sum_{i=1}^{N}h_{z}(x_{i})h_{l}(x_{i})\\right)-\\frac{2}{N}\\sum_{k=1}^{K}\\boldsymbol{w}_{k}\\sum h_{k}(x_{i})y_{i}+\\lambda\\parallel W\\parallel_{0}\\right]$$\n",
    "\n",
    "And then we can solve it with a quantum annealer or by using QAOA. So here, I'm just rewriting this expression in a different way. So what I do is I expand this square loss $$\\left(\\sum_{k=1}^{K}\\boldsymbol{w}_{z}h_{z}(x_{i})-y_{i}\\right)^{z}$$. \n",
    "So now I have this quadratic term $$\\left(\\sum_{k=1}^{K}\\boldsymbol{w}_{z}h_{z}(x_{i})\\right)^{z}$$ over the sum of the ensemble. Then I have this term $$2\\sum_{k=1}^{K}\\boldsymbol{w}_{z}h_{z}(x_{i})y_{i}$$, which measures the correlation between the output of a predictor and the actual label. We have this term $y_{i}^{2}$, which is just the label squared. We can get rid of it because there's no parameter in it. So it's not going to affect our minimization. And finally, we retain the regularization term as it was $\\lambda\\parallel W\\parallel_{0}$. So we can continue working on this equation, and we can rearrange certain terms and move the sums around. So if I write it this way, the first term, then what I get is I end up with this term $$\\left(\\sum_{i=1}^{N}h_{z}(x_{i})h_{l}(x_{i})\\right)$$ which measures the correlation between the individual models. And here $\\boldsymbol{w}_{l}$ you have the corresponding weights between the different models. Then-- this should be wl here. So it's a multiplication of two weight vectors-- sorry, two entries in the weight vector. Then you have this term $$\\frac{2}{N}\\sum_{k=1}^{K}\\boldsymbol{w}_{k}\\sum h_{k}(x_{i})y_{i}+\\lambda\\parallel W\\parallel_{0}$$ where you measure the same thing as before $h_{k}(x_{i})y_{i}$. So this is the same kind of correlation between the label and the actual model weighted by the actual weight in the weight vector plus this regularization term $\\lambda\\parallel W\\parallel_{0}$. So in the regularization term you can get rid of this square root 0 $\\sqrt{\\sum W_{i}^{0}}$. It doesn't make any difference. So now if you look at this part $$\\sum_{k=1}^{K}\\boldsymbol{w}_{k}\\sum h_{k}(x_{i})y_{i}+\\lambda\\parallel W\\parallel_{0}$$, what you see here is that you have elements of the W vector weighted by some numbers $h_{k}(x_{i})y_{i}$. So this is the bias term in the Hamiltonian encoding in the Ising model $\\sum_{i}h_{i}\\sigma_{i}$. So this would correspond to the external magnetic field in the Ising model. Well, the only difference is that the WK entries are described as bit strings whereas here, we have spins, which take values plus or minus. But that's just the shift. That's very easy to transform. And then this term here has this interaction, this quadratic interaction, between these discrete variables. So this maps to the $\\sigma_I$, $\\sigma_J$ interaction with certain couplings. So here you have your Ising model. That's all you have to do. And now you can solve this ensemble problem on a quantum computer.\n",
    "\n",
    "Rewind this video!!!\n",
    "\n",
    "Check:\n",
    "\n",
    "• Why do we consider $l_{0}$-regularization?\n",
    "\n",
    "– It is a stronger constraint on sparsity than $l_{1}$ or $l_{2}$ normalization.\n",
    "\n",
    "• We use a low bit width representation of the weights because...\n",
    "\n",
    "– Current quantum computers have few qubits."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RuA7UH_LTUou"
   },
   "source": [
    "# More QBoost\n",
    "\n",
    "The idea of Qboost is that optimization on a quantum computer is not constrained to convex objective functions, therefore we can add arbitrary penalty terms and rephrase our objective [[1](#1)]. Qboost solves the following problem:\n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}_{w} \\left(\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\sum_{k=1}^{K}w_kh_k(x_i)-\n",
    "y_i\\right)^2+\\lambda\\|w\\|_0\\right),\n",
    "$$\n",
    "\n",
    "where $h_k(x_i)$ is the prediction of the weak learner $k$ for a training instance $k$. The weights in this formulation are binary, so this objective function already maps to an Ising model. The regularization in the $l_0$ norm ensures sparsity, and it is not the kind of regularization we would consider classically: it is hard to optimize with this term on a digital computer.\n",
    "\n",
    "Let us expand the quadratic part of the objective:\n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}_{w} \\left(\\frac{1}{N}\\sum_{i=1}^{N}\n",
    "\\left( \\left(\\sum_{k=1}^{K} w_k h_k(x_i)\\right)^{2} -\n",
    "2\\sum_{k=1}^{K} w_k h_k(\\mathbf{x}_i)y_i + y_i^{2}\\right) + \\lambda \\|w\\|_{0}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Since $y_i^{2}$ is just a constant offset, the optimization reduces to\n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}_{w} \\left(\n",
    "\\frac{1}{N}\\sum_{k=1}^{K}\\sum_{l=1}^{K} w_k w_l\n",
    "\\left(\\sum_{i=1}^{N}h_k(x_i)h_l(x_i)\\right) - \n",
    "\\frac{2}{N}\\sum_{k=1}^{K}w_k\\sum_{i=1}^{N} h_k(x_i)y_i +\n",
    "\\lambda \\|w\\|_{0} \\right).\n",
    "$$\n",
    "\n",
    "This form shows that we consider all correlations between the predictions of the weak learners: there is a summation of $h_k(x_i)h_l(x_i)$. Since this term has a positive sign, we penalize for correlations. On the other hand, the correlation with the true label, $h_k(x_i)y_i$, has a negative sign. The regularization term remains unchanged.\n",
    "\n",
    "\n",
    "\n",
    "To run this on an annealing machine we discretize this equation, reduce the weights to single bits, and normalize the estimator by K to scale with the feature data. As the weights are single bit, the regularization term becomes a summation that allows us to turn the expression into a QUBO.\n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}_{w} \\sum_{k=1}^{K} \\sum_{l=1}^{K}  w_kw_l \\sum_{i=1}^{N}\\frac{1}{K^2}h_k(x_i)h_l(x_i) + \\sum_{k=1}^{K}w_k \\left(\\lambda-2\\sum_{i=1}^{N} \\frac{1}{K}h_k(x_i)y_i   \\right), \\mathrm{w}_k \\in \\{0,1\\}\n",
    "$$\n",
    "\n",
    "We split off the diagonal coefficients (k=l) in the left term and since $\\mathrm {w}\\in \\{0,1\\}$, and predictions, $\\mathrm h_k(x_i) \\in\\{-1,1\\}$ the following holds:\n",
    "\n",
    "$$\n",
    "w_kw_k = w_k,\\;h_k(x_i)h_k(x_i) = 1\n",
    "$$\n",
    "\n",
    "Hence:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathrm \\sum_{k=1}^{K}  w_kw_k \\sum_{i=1}^{N}\\frac{1}{K^2}h_k(x_i)h_k(x_i) = \\sum_{k=1}^{K}  w_k \\frac{N}{K^2}\n",
    "$$\n",
    "\n",
    "This last term is effectively a fixed offset to $\\lambda $ \n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}_{w} \\sum_{k\\neq1}^{K}  w_kw_l \\left(\\sum_{i=1}^{N}\\frac{1}{K^2}h_k(x_i)h_l(x_i)\\right) + \\sum_{k=1}^{K}w_k \\left(\\frac{N}{K^2} +\\lambda-2\\sum_{i=1}^{N} \\frac{1}{K}h_k(x_i)y_i   \\right), \\mathrm{w}_k \\in \\{0,1\\}\n",
    "$$\n",
    "\n",
    "The expressions between brackets are the coeficients of the QUBO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDE_QJu3TUov"
   },
   "source": [
    "\n",
    "Let us consider all three models from the previous section as weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.320974Z",
     "start_time": "2018-11-19T20:10:18.316633Z"
    },
    "id": "tGr5TO2_TUow",
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = [model_1, model_2, model_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbygvLQtTUow"
   },
   "source": [
    "We calculate their predictions and set $\\lambda$ to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.354723Z",
     "start_time": "2018-11-19T20:10:18.323802Z"
    },
    "id": "YvK86jagTUow",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_models = len(models)\n",
    "predictions = np.array([h.predict(X_train) for h in models], dtype=np.float64)\n",
    "λ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDHjhEsFTUox"
   },
   "source": [
    "We create the quadratic binary optimization of the objective function as we expanded above.\n",
    "First the off-diagonal elements (see DWave's documentation for the sample_qubo() method ):\n",
    "\n",
    "$$\n",
    "q_{ij} = \\sum_{i=1}^{N}\\frac{1}{K^2}h_k(x_i)h_l(x_i) \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.375760Z",
     "start_time": "2018-11-19T20:10:18.357248Z"
    },
    "id": "_oOSdcyrTUoy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "q = predictions @ predictions.T/(n_models ** 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2HjVH5XTUoy"
   },
   "source": [
    "Then the diagonal elements:\n",
    "\n",
    "$$\n",
    "\\mathrm q_{ii} =\\frac{N}{K^2}+ \\lambda-2\\sum_{i=1}^{N} \\frac{1}{K}h_k(x_i)y_i\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "RcxYft03TUoz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "qii = len(X_train) / (n_models ** 2) + λ - 2 * predictions @ y_train/(n_models)\n",
    "\n",
    "q[np.diag_indices_from(q)] = qii\n",
    "Q = {}\n",
    "for i in range(n_models):\n",
    "    for j in range(i, n_models):\n",
    "        Q[(i, j)] = q[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptVGWE6_TUoz"
   },
   "source": [
    "We solve the quadratic binary optimization with simulated annealing and read out the optimal weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.703378Z",
     "start_time": "2018-11-19T20:10:18.378217Z"
    },
    "id": "MdsK3tCxTUoz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dimod\n",
    "sampler = dimod.SimulatedAnnealingSampler()\n",
    "response = sampler.sample_qubo(Q, num_reads=10)\n",
    "weights = list(response.first.sample.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMooYwvkTUo0"
   },
   "source": [
    "We define a prediction function to help with measuring accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.715360Z",
     "start_time": "2018-11-19T20:10:18.705496Z"
    },
    "id": "n9oLTe0cTUo0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(models, weights, X):\n",
    "\n",
    "    n_data = len(X)\n",
    "    T = 0\n",
    "    y = np.zeros(n_data)\n",
    "    for i, h in enumerate(models):\n",
    "        y0 = weights[i] * h.predict(X)  # prediction of weak classifier\n",
    "        y += y0\n",
    "        T += np.sum(y0)\n",
    "\n",
    "    y = np.sign(y - T / (n_data*len(models)))\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.734604Z",
     "start_time": "2018-11-19T20:10:18.719931Z"
    },
    "id": "bxZJMiNbTUo1",
    "outputId": "6d14600d-8aaa-4198-a84d-6a5b4efa8596"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  0.65\n",
      "accuracy (test):  0.29\n"
     ]
    }
   ],
   "source": [
    "print('accuracy (train): %5.2f'%(metric(y_train, predict(models, weights, X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, predict(models, weights, X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e_H5l-FTUo1"
   },
   "source": [
    "The accuracy co-incides with our strongest weak learner's, the AdaBoost model. Looking at the optimal weights, this is apparent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.751765Z",
     "start_time": "2018-11-19T20:10:18.736771Z"
    },
    "id": "RPJx3fJaTUo1",
    "outputId": "dfe7ca24-6af6-451e-89e6-3074bd39f031"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcIy-pQ6TUo2"
   },
   "source": [
    "Only AdaBoost made it to the final ensemble. The first two models perform poorly and their predictions are correlated. Yet, if you remove regularization by setting $\\lambda=0$ above, the second model also enters the ensemble, decreasing overall performance. This shows that the regularization is in fact important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IF_HWbIXTUo2"
   },
   "source": [
    "# Solving by QAOA\n",
    "\n",
    "Since eventually our problem is just an Ising model, we can also solve it on a gate-model quantum computer by QAOA. Let us explicitly map the binary optimization to the Ising model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.765328Z",
     "start_time": "2018-11-19T20:10:18.754605Z"
    },
    "id": "ZD03v2KJTUo3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "h, J, offset = dimod.qubo_to_ising(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8cAOVtPTUo3"
   },
   "source": [
    "We have to translate the Ising couplings to be suitable for solving by the QAOA routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:19.838597Z",
     "start_time": "2018-11-19T20:10:18.767740Z"
    },
    "id": "JZ3LUDdxTUo3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/93/6sy7vf8142969b_8w873sxcc0000gn/T/ipykernel_93780/2541997201.py:10: DeprecationWarning: Passing 'z' and 'x' arrays separately to 'Pauli' is deprecated as of Qiskit Terra 0.17 and will be removed in version 0.23 or later. Use a tuple instead, such as 'Pauli((z, x[, phase]))'.\n",
      "  pauli_list.append([h[i], Pauli(vp, wp)])\n",
      "/var/folders/93/6sy7vf8142969b_8w873sxcc0000gn/T/ipykernel_93780/2541997201.py:17: DeprecationWarning: Passing 'z' and 'x' arrays separately to 'Pauli' is deprecated as of Qiskit Terra 0.17 and will be removed in version 0.23 or later. Use a tuple instead, such as 'Pauli((z, x[, phase]))'.\n",
      "  pauli_list.append([J[i, j], Pauli(vp, wp)])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Operator.__init__() got an unexpected keyword argument 'paulis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             vp[j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     17\u001b[0m             pauli_list\u001b[38;5;241m.\u001b[39mappend([J[i, j], Pauli(vp, wp)])\n\u001b[0;32m---> 18\u001b[0m ising_model \u001b[38;5;241m=\u001b[39m \u001b[43mOperator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaulis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpauli_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Operator.__init__() got an unexpected keyword argument 'paulis'"
     ]
    }
   ],
   "source": [
    "#from qiskit.quantum_info import Pauli\n",
    "#from qiskit.aqua import Operator\n",
    "from qiskit.quantum_info.operators import Operator, Pauli\n",
    "num_nodes = q.shape[0]\n",
    "pauli_list = []\n",
    "for i in range(num_nodes):\n",
    "    wp = np.zeros(num_nodes)\n",
    "    vp = np.zeros(num_nodes)\n",
    "    vp[i] = 1\n",
    "    pauli_list.append([h[i], Pauli(vp, wp)])\n",
    "    for j in range(i+1, num_nodes):\n",
    "        if q[i, j] != 0:\n",
    "            wp = np.zeros(num_nodes)\n",
    "            vp = np.zeros(num_nodes)\n",
    "            vp[i] = 1\n",
    "            vp[j] = 1\n",
    "            pauli_list.append([J[i, j], Pauli(vp, wp)])\n",
    "ising_model = Operator(paulis=pauli_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qbTjL9ETUo4"
   },
   "source": [
    "Next we run the optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:40.568546Z",
     "start_time": "2018-11-19T20:10:19.840830Z"
    },
    "collapsed": true,
    "id": "ZBYWYuNaTUo4",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from qiskit.aqua import get_aer_backend, QuantumInstance\n",
    "from qiskit.aqua.algorithms import QAOA\n",
    "from qiskit.aqua.components.optimizers import COBYLA\n",
    "p = 1\n",
    "optimizer = COBYLA()\n",
    "qaoa = QAOA(ising_model, optimizer, p, operator_mode='matrix')\n",
    "backend = get_aer_backend('statevector_simulator')\n",
    "quantum_instance = QuantumInstance(backend, shots=100)\n",
    "result = qaoa.run(quantum_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3xvcae8TUo4"
   },
   "source": [
    "Finally, we extract the most likely solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:40.577140Z",
     "start_time": "2018-11-19T20:10:40.571807Z"
    },
    "collapsed": true,
    "id": "S-4cTGdyTUo5",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "k = np.argmax(result['eigvecs'][0])\n",
    "weights = np.zeros(num_nodes)\n",
    "for i in range(num_nodes):\n",
    "    weights[i] = k % 2\n",
    "    k >>= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-_2Vp1wTUo5"
   },
   "source": [
    "Let's see the weights found by QAOA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:40.597309Z",
     "start_time": "2018-11-19T20:10:40.579449Z"
    },
    "id": "aSwriIu0TUo6",
    "outputId": "3d65b7e8-23be-4f6a-e1af-d928ef549df1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siz2ZWTPTUo6"
   },
   "source": [
    "And the final accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:40.614781Z",
     "start_time": "2018-11-19T20:10:40.602793Z"
    },
    "id": "3mWOF0cKTUo6",
    "outputId": "9dc05e95-5f4a-41e3-dbc6-39d05e9e81d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  0.64\n",
      "accuracy (test):  0.24\n"
     ]
    }
   ],
   "source": [
    "print('accuracy (train): %5.2f'%(metric(y_train, predict(models, weights, X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, predict(models, weights, X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUgVsfIpTUo7"
   },
   "source": [
    "# References\n",
    "\n",
    "[1] Neven, H., Denchev, V.S., Rose, G., Macready, W.G. (2008). [Training a binary classifier with the quantum adiabatic algorithm](https://arxiv.org/abs/0811.0416). *arXiv:0811.0416*.  <a id='1'></a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3_1_QML_Classical-Quantum Hybrid Learning Algorithms_(9):_Discrete_Optimization_And_Ensemble_Learning-qiskit(Ecoding_Classical_Inforfmation).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
